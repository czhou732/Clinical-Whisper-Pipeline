# ============================================================
# ClinicalWhisper Configuration
# ============================================================

# Whisper model: tiny.en | base.en | small.en | medium.en | large-v3
model: "small.en"
# MLX-optimized model repo (Apple Silicon native — 4-10x faster)
mlx_model: "mlx-community/whisper-small.en"


# Folders (relative to this file, or absolute paths)
input_folder: "./Input"
processed_folder: "./Processed"

# Where to save Markdown notes
output_folder: "/Users/peterzhou/Research/Writing/PhDApplications_2026/EducationRecords/Education/USC/USC/ClinicalWhisper_Audio"

# Supported audio extensions
audio_extensions:
  - ".m4a"
  - ".mp3"
  - ".wav"
  - ".mp4"

# Sentiment analysis
sentiment:
  enabled: true
  sentences_per_segment: 5   # how many sentences per analysis chunk

# Transcript statistics
statistics:
  enabled: true               # word count, duration estimate, etc.

# Acoustic features
acoustic_features:
  enabled: true               # extracts F0, Energy, VTA via OpenSMILE eGeMAPSv02

# Duplicate protection
skip_already_processed: true   # skip files that already have a note in output

# Speaker diarization (identifies WHO is speaking)
# Requires: pip install pyannote.audio
# And a HuggingFace token from https://huggingface.co/settings/tokens
# You must also accept terms at https://huggingface.co/pyannote/speaker-diarization-3.1
diarization:
  enabled: true
  hf_token: ""                 # leave empty — set HF_TOKEN env var instead (GitHub blocks tokens in repos)
  min_speakers: 2
  max_speakers: 6

# Async pipeline (ingestion API + queue + background workers)
pipeline:
  queue_db_path: "./clinicalwhisper_jobs.db"
  secure_storage_folder: "./Input/ingestion"
  analysis_output_folder: "/Users/peterzhou/Research/Writing/PhDApplications_2026/EducationRecords/Education/USC/USC/ClinicalWhisper_Audio"
  poll_interval_seconds: 1.0
  max_workers: 1

# Plaud Note integration
plaud:
  enabled: true
  # Directories to watch for Plaud audio/transcription files
  watch_paths:
    - "/Volumes/PLAUD*"              # USB mount (glob pattern)
    - "~/Downloads/PlaudExports"     # App export folder
  # How to submit to Clinical Whisper: "api" or "direct"
  # "direct" = copies into Input/ for main.py watcher (no API server needed)
  ingestion_mode: "direct"
  api_url: "http://127.0.0.1:8000/jobs"
  # Audio extensions to ingest
  audio_extensions: [".mp3", ".wav", ".m4a"]
  # Optional: also ingest text transcription files
  ingest_transcriptions: false
  transcript_extensions: [".txt", ".srt"]
  # Deduplication DB
  db_path: "./plaud_ingested.db"
  # Poll interval for USB mount detection (seconds)
  usb_poll_interval: 5
  # Archive ingested files instead of leaving them
  archive_after_ingest: true
  archive_folder: "./PlaudArchive"

# Meeting Intelligence — local LLM meeting summary extraction
# Run: python meeting_intel.py --file path/to/transcript.md
meeting_intel:
  enabled: true
  # Must be pulled locally first: ollama pull gpt-oss:20b
  ollama_model: "gpt-oss:20b"
  ollama_base_url: "http://localhost:11434"
  # Timeout in seconds — gpt-oss:20b on 4,000-word transcripts takes ~60-120s
  timeout_seconds: 180
  # Retry once on timeout (connection errors are not retried)
  max_retries: 1
  # Output filename: {original_stem}{summary_suffix}.md
  summary_suffix: "_summary"
  # Skip transcripts shorter than this many words
  min_word_count: 50
